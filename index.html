<html>
<head>
  <meta content="text/html; charset=utf-8" http-equiv="Content-Type" />
  <title>Peng-Tao Jiang</title>
  <meta content="Peng-Tao Jiang, pengtaojiang.github.io" name="keywords" />
  <style media="screen" type="text/css">html, body, div, span, applet, object, iframe, h1, h2, h3, h4, h5, h6, p, blockquote, pre, a, abbr, acronym, address, big, cite, code, del, dfn, em, font, img, ins, kbd, q, s, samp, small, strike, strong, sub, tt, var, dl, dt, dd, ol, ul, li, fieldset, form, label, legend, table, caption, tbody, tfoot, thead, tr, th, td {
  border: 0pt none;
  font-family: inherit;
  font-size: 100%;
  font-style: inherit;
  font-weight: inherit;
  margin: 0pt;
  outline-color: invert;
  outline-style: none;
  outline-width: 0pt;
  padding: 0pt;
  vertical-align: baseline;
}
a {
  color: #1772d0;
  text-decoration:none;
}
a:focus, a:hover {
  color: #f09228;
  text-decoration:none;
}
a.paper {
  font-weight: bold;
  font-size: 14pt;
}
b.paper {
  font-weight: bold;
  font-size: 14pt;
}
* {
  margin: 0pt;
  padding: 0pt;
}
body {
  position: relative;
  margin: 3em auto 2em auto;
  width: 1000px;
  font-family: Lato, Verdana, Helvetica, sans-serif;
  font-size: 16px;
  background: #eee;
}
h2 {
  font-family: Lato, Verdana, Helvetica, sans-serif;
  font-size: 17pt;
  font-weight: 700;
}
h3 {
  font-family: Lato, Verdana, Helvetica, sans-serif;
  font-size: 18px;
  font-weight: 700;
}
strong {
  font-family: Lato, Verdana, Helvetica, sans-serif;
  font-size: 15px;
  font-weight:bold;
}
ul { 
  list-style: circle;
}
img {
  border: none;
}
li {
  padding-bottom: 0.5em;
  margin-left: 1.4em;
}
alert {
  font-family: Lato, Verdana, Helvetica, sans-serif;
  font-size: 15px;
  font-weight: bold;
  color: #FF0000;
}
em, i {
  font-style:italic;
}
div.section {
  clear: both;
  margin-bottom: 1.5em;
  background: #eee;
}
div.spanner {
  clear: both;
}
div.paper {
  clear: both;
  margin-top: 0.5em;
  margin-bottom: 1em;
  border: 1px solid #ddd;
  background: #fff;
  padding: 1em 1em 1em 1em;
}
div.paper div {
  padding-left: 230px;
}
img.paper {
  margin-bottom: 0.5em;
  float: left;
  width: 200px;
}
span.blurb {
  font-style:italic;
  display:block;
  margin-top:0.75em;
  margin-bottom:0.5em;
}
pre, code {
  font-family: 'Lucida Console', 'Andale Mono', 'Courier', monospaced;
  margin: 1em 0;
  padding: 0;
}
div.paper pre {
  font-size: 0.9em;
}
</style>

<link href="http://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic" rel="stylesheet" type="text/css" /><!--<link href='http://fonts.googleapis.com/css?family=Open+Sans+Condensed:300' rel='stylesheet' type='text/css'>--><!--<link href='http://fonts.googleapis.com/css?family=Open+Sans' rel='stylesheet' type='text/css'>--><!--<link href='http://fonts.googleapis.com/css?family=Yanone+Kaffeesatz' rel='stylesheet' type='text/css'>-->
</head>
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-45959174-3', 'kailigo.github.io');
  ga('send', 'pageview');
</script>
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-66888300-1', 'auto');
  ga('send', 'pageview');
</script>
<body>
<div style="margin-bottom: 1em; border: 1px solid #ddd; background-color: #fff; padding: 1em; height: 140px;">
<div style="margin: 0px auto; width: 100%;">
<img title="Peng-Tao" style="float: left; padding-left: .5em; height: 140px;" src="jpt2.jpeg" />
<div style="padding-left: 12em; vertical-align: top; height: 120px;"><span style="line-height: 150%; font-size: 20pt;">Peng-Tao Jiang</span><br />
<span><strong>vivo researcher</strong></span><br />
<span><strong>School  </strong>: Nankai University </span><br />  
<span><strong>Email  </strong>: pt.jiang [at] vivo.com</span><br />  
<span><strong>Email  </strong>: pt.jiang [at] mail.nankai.edu.cn </span><br />  
<strong><a href='https://github.com/PengtaoJiang'>Github</a></strong>  &nbsp &nbsp
<strong><a href='https://scholar.google.com/citations?user=85QJ_i4AAAAJ&hl=en'>Google Scholar</a></strong></span><br/>
</div>
</div>
</div>
<!--<div style="clear: both; background-color: #fff; margin-top: 1.5em; padding: .2em; padding-left: .3em;">-->

<div style="clear: both;">
<div class="section">
<h2>Biography</h2>
<div class="paper">
My name is Peng-Tao Jiang (姜鹏涛). Currently, I am a lead researcher&engineer in the <a href=https://vivocamera.github.io/>Quality Enhancement Center of vivo</a>. Before that, I was a post-doc researcher at Zhejiang University, 
working with <a href=https://cshen.github.io/>Prof. Chunhua Shen</a>. Before that, I received my PhD at Nankai University, advised by <a href=https://mmcheng.net/zh/cmm/>Prof. Ming-Ming Cheng</a>. 
Moreover, I have spent several months as an intern at <a href=https://www.sensetime.com/cn/>SenseTime</a> and <a href=https://open.youtu.qq.com/#/open>Tencent YouTu</a>.
My research interests include diffusion for image restoration, multi-task learning, segmentation and open-vocabulary learning and etc.
</div>

<div style="clear: both;">
<div class="section">
<h2>Experience</h2>
<div class="paper">
<strong>2017-2022:</strong>  Computer Science in Nankai University <br>
<strong>2022-2023:</strong>  Post-doc Researcher in Zhejiang University <br>
<strong>2023-now:</strong>  Researcher in vivo 
</div>


<div style="clear: both;">
<div class="section">
<h2>Hiring</h2>
<div class="paper">
Our group focuses on several topics: diffusion, low-level vision, 
multi-modal large language models, multi-task learning, segmentation and detection etc. 
We are hiring self-motivated research interns at vivo. If you are interested in our group, 
please feel free to contact us.     
</div>


<div style="clear: both;">
  <div class="section">
  <h2> Pre-print </h2>
  
  <div class="paper" id="semat"><img class="paper" src="./imgs/semat.jpg" title="Towards Natural Image Matting in the Wild via Real-Scenario Prior">
    <div> <strong>Towards Natural Image Matting in the Wild via Real-Scenario Prior</strong><br>
      Ruihao Xia, Yu Liang, Peng-Tao Jiang#, Hao Zhang, Qianru Sun, Yang Tang#, Bo Li, Pan Zhou<br>
      arxiv, 2024 <br>
      <a href='https://github.com/XiaRho/SEMat'>[PDF]</a> 
      <a href='https://github.com/XiaRho/SEMat'>[CODE]</a> <br>
    </div>
    <div class="spanner"></div>
  </div>
  
  <div class="paper" id="lsg"><img class="paper" src="./imgs/lsg.png" title="Segment Anything is A Good Pseudo-label Generator
    for Weakly Supervised Semantic Segmentation">
      <div> <strong>Segment Anything is A Good Pseudo-label Generator for Weakly Supervised Semantic Segmentation</strong><br>
      <strong><u>Peng-Tao Jiang*</u></strong>, Yuqi Yang*<br>
      arXiv, 2023 <br>
      <a href='https://arxiv.org/pdf/2305.01275.pdf'>[PDF]</a> 
      <a href=''>[CODE]</a> <br>
    </div>
    <div class="spanner"></div>
  </div>

  

<div style="clear: both;">
<div class="section">
<h2> Publications </h2>
* denotes equal contribution.

<!-- <div class="paper" id="bttdm"><img class="paper" src="./imgs/bttdm.jpg" title="Beta-Tuned Timestep Diffusion Model">
  <div> <strong>Beta-Tuned Timestep Diffusion Model</strong><br>
    Lv Tang*, <strong><u>Peng-Tao Jiang*</u></strong>, Haoke Xiao*, Bo Li#<br>
    (<strong>IJCV</strong>), 2024 <br>
    <a href='https://arxiv.org/pdf/2310.10912'>[PDF]</a> 
    <a href=''>[CODE]</a> <br>
  </div>
  <div class="spanner"></div>
</div> -->

<div class="paper" id="mmcpf"><img class="paper" src="./imgs/mmcpf.jpg" title="Chain of Visual Perception: Harnessing Multimodal Large Language Models for Zero-shot Camouflaged Object Detection">
  <div> <strong>Chain of Visual Perception: Harnessing Multimodal Large Language Models for Zero-shot Camouflaged Object Detection</strong><br>
    Lv Tang, <strong><u>Peng-Tao Jiang</u></strong>, Zhihao Shen, Hao Zhang, Jinwei Chen, Bo Li<br>
    ACM International Conference on Multimedia(<strong>MM</strong>), 2024 <br>
    <a href='https://arxiv.org/pdf/2311.11273'>[PDF]</a> 
    <a href='https://github.com/luckybird1994/MMCPF'>[CODE]</a> <br>
  </div>
  <div class="spanner"></div>
</div>

<div class="paper" id="ddpmbs"><img class="paper" src="./imgs/ddpmbs.jpg" title="Non-uniform Timestep Sampling: Towards Faster Diffusion Model Training">
  <div> <strong>Non-uniform Timestep Sampling: Towards Faster Diffusion Model Training</strong><br>
    Tianyi Zheng, Cong Geng, <strong><u>Peng-Tao Jiang</u></strong>, Ben Wan, Hao Zhang, Jinwei Chen, Jia Wang#, Bo Li#<br>
    ACM International Conference on Multimedia(<strong>MM</strong>), 2024 <br>
    <a href='https://openreview.net/pdf?id=NQPJYEyiiM'>[PDF]</a> 
    <a href=''>[CODE]</a> <br>
  </div>
  <div class="spanner"></div>
</div>

<div class="paper" id="bttdm"><img class="paper" src="./imgs/bttdm.jpg" title="Beta-Tuned Timestep Diffusion Model">
  <div> <strong>Beta-Tuned Timestep Diffusion Model</strong><br>
    Tianyi Zheng, <strong><u>Peng-Tao Jiang</u></strong>, Ben Wan, Hao Zhang, Jinwei Chen, Jia Wang#, Bo Li#<br>
    European Conference on Computer Vision(<strong>ECCV</strong>), 2024 <br>
    <a href='https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/00328.pdf'>[PDF]</a> 
    <a href=''>[CODE]</a> <br>
  </div>
  <div class="spanner"></div>
</div>

<div class="paper" id="ipseg"><img class="paper" src="./imgs/ipseg.jpg" title="Towards Training-free Open-world Segmentation via Image Prompt Foundation Models">
  <div> <strong>Towards Training-free Open-world Segmentation via Image Prompt Foundation Models</strong><br>
    Lv Tang*, <strong><u>Peng-Tao Jiang*</u></strong>, Haoke Xiao*, Bo Li#<br>
    International Journal of Computer Vision(<strong>IJCV</strong>), 2024 <br>
    <a href='https://arxiv.org/pdf/2310.10912'>[PDF]</a> 
    <a href=''>[CODE]</a> <br>
  </div>
  <div class="spanner"></div>
</div>

<div class="paper" id="ddaebm"><img class="paper" src="./imgs/ddaebm.jpg" title="Improving Adversarial Energy-Based Model via Diffusion Process">
  <div> <strong>Improving Adversarial Energy-Based Model via Diffusion Process</strong><br>
    Cong Geng, Tian Han, <strong><u>Peng-Tao Jiang</u></strong>, Hao Zhang, Jinwei Chen, Søren Hauberg, Bo Li<br>
    International Conference on Machine Learning(<strong>ICML</strong>), 2024 <br>
    <a href='https://arxiv.org/pdf/2403.01666'>[PDF]</a> 
    <a href=''>[CODE]</a> <br>
  </div>
  <div class="spanner"></div>
</div>

<div class="paper" id="rrw"><img class="paper" src="./imgs/rrw.jpg" title="Revisiting Single Image Reflection Removal In the Wild">
  <div> <strong>Revisiting Single Image Reflection Removal In the Wild</strong><br>
    Yurui Zhu, Xueyang Fu, <strong><u>Peng-Tao Jiang</u></strong>, Hao Zhang, Qibin Sun,  Jinwei Chen,  Zheng-Jun Zh, Bo Li<br>
    IEEE Conference on Computer Vision and Pattern Recognition(<strong>CVPR</strong>), 2024 <br>
    <a href='https://arxiv.org/pdf/2311.17320.pdf'>[PDF]</a> 
    <a href=''>[CODE]</a> <br>
  </div>
  <div class="spanner"></div>
</div>

<div class="paper" id="mlore"><img class="paper" src="./imgs/mlore.jpg" title="Multi-Task Dense Prediction via Mixture of Low-Rank Experts">
  <div> <strong>Multi-Task Dense Prediction via Mixture of Low-Rank Experts</strong><br>
    Yuqi Yang*, <strong><u>Peng-Tao Jiang*</u></strong>, Qibin Hou, Hao Zhang, Jinwei Chen, Bo Li<br>
    IEEE Conference on Computer Vision and Pattern Recognition(<strong>CVPR</strong>), 2024 <br>
    <a href='https://arxiv.org/pdf/2403.17749v1.pdf'>[PDF]</a> 
    <a href='https://github.com/YuqiYang213/MLoRE'>[CODE]</a> <br>
  </div>
  <div class="spanner"></div>
</div>


<div class="paper" id="tsp6k"><img class="paper" src="./imgs/tsp6k.png" title="Traffic Scene Parsing through the TSP6K Dataset">
  <div> <strong>Traffic Scene Parsing through the TSP6K Dataset</strong><br>
    <strong><u>Peng-Tao Jiang*</u></strong>, Yuqi Yang*, Yang Cao, Qibin Hou, Ming-Ming Cheng, Chunhua Shen<br>
    IEEE Conference on Computer Vision and Pattern Recognition(<strong>CVPR</strong>), 2024 <br>
    <a href='https://arxiv.org/pdf/2303.02835.pdf'>[PDF]</a> 
    <a href='https://github.com/PengtaoJiang/TSP6K/'>[CODE]</a> 
    <a href='https://www.jianguoyun.com/p/DZVO0kMQ2tnWChjun6MFIAA'>[DATASET(password:Wi9qFT)]</a><br>
  </div>
  <div class="spanner"></div>
</div>


<div class="paper" id="hsr"><img class="paper" src="./imgs/hsr.png" title="Looking Through the Glass: Neural Surface Reconstruction Against High Specular Reflections">
  <div> <strong>Looking Through the Glass: Neural Surface Reconstruction Against High Specular Reflections</strong><br>
    Jiaxiong Qiu, <strong><u>Peng-Tao Jiang</u></strong>, Yifan Zhu, Ze-Xin Yin, Ming-Ming Cheng, Bo Ren<br>
    IEEE Conference on Computer Vision and Pattern Recognition(<strong>CVPR</strong>), 2023 <br>
    <a href='http://ren-bo.net/papers/qjx_cvpr2023.pdf'>[PDF]</a> 
    <a href='https://github.com/JiaxiongQ/NeuS-HSR'>[CODE]</a> <br>
  </div>
  <div class="spanner"></div>
</div>

<div class="paper" id="rd"><img class="paper" src="./imgs/rd.png" title="RDNeRF: Relative Depth Guided NeRF for Dense Free View Synthesis">
  <div> <strong>RDNeRF: Relative Depth Guided NeRF for Dense Free View Synthesis</strong><br>
    Jiaxiong Qiu*, Yifan Zhu*, <strong><u>Peng-Tao Jiang</u></strong>, Ming-Ming Cheng, Bo Ren<br>
    The Visual Computer Journal(<strong>TVC</strong>), 2023 <br>
    <a href='https://trebuchet.public.springernature.app/get_content/6eca4bb0-2124-40dd-858c-8a5aabe5dd14'>[PDF]</a> 
    <a href='https://github.com/JiaxiongQ/RDNeRF'>[CODE]</a> <br>
  </div>
  <div class="spanner"></div>
</div>


<div class="paper" id="hd"><img class="paper" src="./imgs/hd.png" title="Deeply Explain CNN via Hierarchical Decomposition">
  <div> <strong>Deeply Explain CNN via Hierarchical Decomposition</strong><br>
    Ming-Ming Cheng*, <strong><u>Peng-Tao Jiang*</u></strong>, Ling-Hao Han, Liang Wang, Philip Torr<br>
    International Journal of Computer Vision(<strong>IJCV</strong>), 2023 <br>
    <a href='https://mftp.mmcheng.net/Papers/23IJCV_CNNasDT.pdf'>[PDF]</a> 
    <a href='http://mc.nankai.edu.cn/hd/'>[Online Demo]</a> <br>
  </div>
  <div class="spanner"></div>
</div>

<div class="paper" id="l2g"><img class="paper" src="./imgs/l2g.png" title="L2G: A Simple Local-to-Global Knowledge Transfer 
  Framework for Weakly Supervised Semantic Segmentation">
  <div> <strong>L2G: A Simple Local-to-Global Knowledge Transfer Framework for Weakly Supervised Semantic Segmentation</strong><br>
  <strong><u>Peng-Tao Jiang</u></strong>, Yuqi Yang, Qibin Hou, Yunchao Wei<br>
  IEEE Conference on Computer Vision and Pattern Recognition(<strong>CVPR</strong>), 2022 <br>
  <a href=''>[PDF]</a>
  <a href='https://github.com/PengtaoJiang/L2G'>[CODE]</a> <br>
  </div>
  <div class="spanner"></div>
</div>

<div class="paper" id="am"><img class="paper" src="./imgs/am.png" title="Attention mechanisms in computer vision: A survey">
  <div> <strong>Attention mechanisms in computer vision: A survey</strong><br>
  Meng-Hao Guo, Tian-Xing Xu, Jiang-Jiang Liu, Zheng-Ning Liu, <strong><u>Peng-Tao Jiang</u></strong>, Tai-Jiang Mu, Song-Hai Zhang, Ralph R. Martin, Ming-Ming Cheng, and Shi-Min Hu<br>
  Computational Visual Media Journal(<strong>CVMJ</strong>, <a href='https://www.springer.com/journal/41095/updates/25223320'><strong>Best Paper Award</strong></a>), 2022,  <br>
  <a href='https://arxiv.org/abs/2111.07624'>[PDF]</a>
  <a href='https://github.com/MenghaoGuo/Awesome-Vision-Attentions'>[CODE]</a> <br>
  </div>
  <div class="spanner"></div>
</div>
  

<div class="paper" id="piss"><img class="paper" src="./imgs/piss.png" title="Personalized Image Semantic Segmentation">
  <div> <strong>Personalized Image Semantic Segmentation</strong><br>
  Yu Zhang, Chang-bin Zhang, <strong><u>Peng-Tao Jiang</u></strong>, Feng Mao, Ming-Ming Cheng<br>
  IEEE/CVF International Conference on Computer Vision (<strong>ICCV</strong>), 2021 <br>
  <a href='https://arxiv.org/pdf/2107.13978'>[PDF]</a>
  <a href='https://github.com/zhangyuygss/PIS'>[CODE]</a> <br>
  </div>
  <div class="spanner"></div>
</div>

<div class="paper" id="oaa++"><img class="paper" src="./imgs/oaa++.png" title="Online Attention Accumulation for Weakly Supervised Semantic Segmentation">
  <div> <strong>Online Attention Accumulation for Weakly Supervised Semantic Segmentation</strong><br>
  <strong><u>Peng-Tao Jiang*</u></strong>, Ling-Hao Han*, Qibin Hou, Ming-Ming Cheng, Yunchao Wei<br>
  IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2021<br>
  <a href='https://mftp.mmcheng.net/Papers/21PAMI-OAA_PAMI.pdf'>[PDF]</a> <br>
  </div>
  <div class="spanner"></div>
  </div>

<div class="paper" id="OLS"><img class="paper" src="./imgs/ols.png" title="Delving Deep into Label Smoothing">
<div> <strong>Delving Deep into Label Smoothing</strong><br>
Chang-bin Zhang*, <strong><u>Peng-Tao Jiang*</u></strong>, Qibin Hou, Yunchao Wei, Qi Han, Zhen Li, Ming-Ming Cheng <br> 
IEEE Transactions on Image Processing (TIP), 2021<br>
<a href='https://arxiv.org/abs/2011.12562'>[PDF]</a>, 
<a href='https://github.com/zhangchbin/OnlineLabelSmoothing'>[CODE]</a> <br>
</div>
<div class="spanner"></div>
</div>

<div class="paper" id="layerCAM"><img class="paper" src="./imgs/layerCAM.png" title="LayerCAM: Exploring Hierarchical Class Activation Maps for Localization">
<div> <strong>LayerCAM: Exploring Hierarchical Class Activation Maps for Localization</strong><br>
<strong><u>Peng-Tao Jiang*</u></strong>, Chang-bin Zhang*, Qibin Hou, Ming-Ming Cheng, Yunchao Wei<br>
IEEE Transactions on Image Processing (TIP 2021)<br>
<a href='http://mftp.mmcheng.net/Papers/21TIP_LayerCAM.pdf'>[PDF]</a>,
<a href='https://github.com/PengtaoJiang/LayerCAM'>[CODE]</a><br>
<br>
<strong><u>Applied to SK Defect Detection Project</u></strong>
</div>
<div class="spanner"></div>
</div>
  
  
<div class="paper" id="oaa"><img class="paper" src="./imgs/oaa.png" title="Integral Object Mining via Online Attention Accumulation">
<div> <strong>Integral Object Mining via Online Attention Accumulation</strong><br>
<strong><u>Peng-Tao Jiang</u></strong>, Qibin Hou, Yang Cao, Ming-Ming Cheng, Yunchao Wei, Hongkai Xiong<br>
IEEE/CVF International Conference on Computer Vision (<strong>ICCV</strong>), 2019 <br>
<a href='https://openaccess.thecvf.com/content_ICCV_2019/papers/Jiang_Integral_Object_Mining_via_Online_Attention_Accumulation_ICCV_2019_paper.pdf'>[PDF]</a>,
<a href='https://github.com/PengtaoJiang/OAA'>[Code]</a>,
<a href='https://mmcheng.net/oaa/'>[Project]</a> 
</div>
<div class="spanner"></div>
</div>
  

<div class="paper" id="SeeNet"><img class="paper" src="./imgs/seenet.png" title="Self-Erasing Network for Integral Object Attention">
<div> <strong>Self-Erasing Network for Integral Object Attention</strong><br>
Qibin Hou, <strong><u>Peng-Tao Jiang</u></strong>, Yunchao Wei, Ming-Ming Cheng <br>
Neural Information Processing Systems (<strong>NeurIPS</strong>), 2018 <br>
<a href='https://papers.nips.cc/paper/2018/file/c042f4db68f23406c6cecf84a7ebb0fe-Paper.pdf'>[PDF]</a>,
<a href='https://github.com/Andrew-Qibin/SeeNet'>[Code]</a>,
<a href='https://mmcheng.net/seenet/'>[Project]</a> 
</div>
<div class="spanner"></div>
</div>


<div class="paper" id="del"><img class="paper" src="./imgs/del.png" title="DEL: Deep Embedding Learning for Efficient Image Segmentation">
<div> <strong>DEL: Deep Embedding Learning for Efficient Image Segmentation</strong><br>
Yun Liu, <strong><u>Peng-Tao Jiang</u></strong>, Vahan Petrosyan, Shi-Jie Li, Jiawang Bian, 
Le Zhang, and Ming-Ming Cheng <br>
International Joint Conferences on Artificial Intelligence (<strong>IJCAI</strong>), 2018 <br> 
<a href='https://www.ijcai.org/Proceedings/2018/0120.pdf'>[PDF]</a>,
<a href='https://github.com/yun-liu/del/'>[Code]</a>
<a href='https://mmcheng.net/del/'>[Project]</a>
</div>
<div class="spanner"></div>
</div>


<!-- <div style="clear: both;">
<div class="section">
<h2> Projects </h2>

<div class="paper" id="blade"><img class="paper" src="./imgs/goldwind.png" title="GOLDWIND: Blade Snow Detection">
<div> <strong>GOLDWIND: Blade Snow Detection</strong><br>
<strong><u>Brief Description:</u></strong>
</div>
<div class="spanner"></div>
</div>


<div class="paper" id="face"><img class="paper" src="./imgs/face.jpg" title="">
<div> <strong>Tianjin Science and Technology Museum: Smiley Face Detection and Style Transfer</strong><br>
<strong><u>Brief Description:</u></strong>
</div>
<div class="spanner"></div>
</div> -->


<!-- <div style="clear: both;">
<div class="section">
<h2 id="confpapers">Awards</h2>
<div class="paper">
    <ul>
  <li> Special Scholarship, Xidian University, 2016. </li>
  <li> Meritorious Winner in International Mathematical Contest in Modeling, 2015. </li>  
  <li> National National Endeavor Scholarship, Xidian University, 2014. </li>
    </ul>
</div>
</div>
</div> -->

<div style="clear: both;">
<div class="section">
<h2 id="confpapers">Professional Activities</h2>
<div class="paper">
<ul>
<p><font size="5">
  <li> Reviewer for TPAMI, TIP, IJCV, IJCAI, IJAC, NeurIPS, CVPR, ICCV, ICML, AAAI, PRCV </li>        
<!-- <div id="clustrmaps-widget"></div><script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=150&t=m&d=IF-jAGUNTygi5pa59hxIgtJU2XqT-rGoO58Z3E1vHZk'></script> -->
 
</body>
</html>
